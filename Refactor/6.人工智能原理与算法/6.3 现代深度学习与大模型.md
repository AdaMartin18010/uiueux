# 6.3 现代深度学习与大模型

[返回6.人工智能原理与算法](./README.md) | [返回Refactor总览](../README.md)

## 目录

- [6.3 现代深度学习与大模型](#63-现代深度学习与大模型)
  - [目录](#目录)
  - [1. 概述](#1-概述)
  - [2. 深度学习发展脉络](#2-深度学习发展脉络)
    - [2.1 早期神经网络与突破](#21-早期神经网络与突破)
    - [2.2 卷积神经网络（CNN）](#22-卷积神经网络cnn)
    - [2.3 循环神经网络（RNN）与序列建模](#23-循环神经网络rnn与序列建模)
    - [2.4 生成模型与自监督学习](#24-生成模型与自监督学习)
  - [3. Transformer与大模型](#3-transformer与大模型)
    - [3.1 Transformer结构与原理](#31-transformer结构与原理)
    - [3.2 预训练-微调范式](#32-预训练-微调范式)
    - [3.3 大语言模型（LLM）与多模态模型](#33-大语言模型llm与多模态模型)
  - [4. 形式化表达与代码示例](#4-形式化表达与代码示例)
  - [5. 相关性引用](#5-相关性引用)
  - [6. 参考文献](#6-参考文献)

---

## 1. 概述

现代深度学习以多层神经网络为核心，推动了计算机视觉、自然语言处理、语音识别等领域的突破。Transformer与大模型（如GPT、BERT、Stable Diffusion等）成为AI发展的新范式。

## 2. 深度学习发展脉络

### 2.1 早期神经网络与突破

- 感知机、多层感知机（MLP）、反向传播算法。
- 激活函数：Sigmoid、ReLU、Tanh。

### 2.2 卷积神经网络（CNN）

- 局部感受野、权重共享、池化层。
- 典型模型：LeNet、AlexNet、ResNet。
- 代码示例（PyTorch）：

```python
import torch.nn as nn
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 32, 3)
        self.pool = nn.MaxPool2d(2)
        self.fc = nn.Linear(32*13*13, 10)
    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv(x)))
        x = x.view(-1, 32*13*13)
        x = self.fc(x)
        return x
```

### 2.3 循环神经网络（RNN）与序列建模

- RNN、LSTM、GRU。
- 序列到序列（Seq2Seq）、注意力机制。

### 2.4 生成模型与自监督学习

- 生成对抗网络（GAN）、变分自编码器（VAE）。
- 自监督学习、对比学习。

## 3. Transformer与大模型

### 3.1 Transformer结构与原理

- 自注意力机制、多头注意力、位置编码。
- 公式：$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
- 编码器-解码器结构。

### 3.2 预训练-微调范式

- 预训练任务：掩码语言模型、因果语言建模。
- 微调：下游任务适配。
- 迁移学习与参数高效微调（如LoRA、Adapter）。

### 3.3 大语言模型（LLM）与多模态模型

- GPT、BERT、T5、PaLM、Llama等。
- 多模态模型：CLIP、Stable Diffusion、SAM。
- 规模化训练、推理加速、分布式并行。

## 4. 形式化表达与代码示例

- 以集合、张量、函数、优化目标等数学符号描述深度学习模型。
- 例：Transformer注意力机制公式、GAN的最小最大优化目标。
- 代码示例：PyTorch、TensorFlow、伪代码。

## 5. 相关性引用

- [6.1 AI基础原理](./6.1%20AI基础原理.md)：神经网络与深度学习基础。
- [2.5 WebAssembly](../2.技术栈与框架/2.5%20WebAssembly.md)：大模型推理的高性能部署。
- [3.5 Dart-Flutter](../3.编程语言范式/3.5%20Dart-Flutter.md)：AI模型在跨端应用中的集成。

## 6. 参考文献

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. Vaswani, A., et al. (2017). Attention is All You Need. NeurIPS.
3. Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL.
4. Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS.
5. Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. ICML.
