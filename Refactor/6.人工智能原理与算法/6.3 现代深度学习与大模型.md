# 6.3 现代深度学习与大模型

[返回6.人工智能原理与算法](6.人工智能原理与算法/README.md) | [返回Refactor总览](6.人工智能原理与算法/../README.md)

---

## 2024前沿趋势

- **多模态大模型**：GPT-4V、Claude 3.5 Sonnet、Gemini 1.5 Pro。
- **开源大模型**：Llama 3、Mistral、Qwen、Yi、DeepSeek。
- **推理优化**：KV Cache、Flash Attention、Paged Attention。
- **训练技术**：LoRA、QLoRA、DoRA、持续预训练。
- **架构创新**：Mamba、RetNet、RWKV、状态空间模型。
- **AI Agent**：AutoGPT、LangChain、ReAct、Tool Calling。

---

## 目录

- [6.3 现代深度学习与大模型](#63-现代深度学习与大模型)
  - [2024前沿趋势](#2024前沿趋势)
  - [目录](#目录)
  - [6.3.1 Transformer架构Mermaid图](#631-transformer架构mermaid图)
  - [6.3.2 深度学习LaTeX公式](#632-深度学习latex公式)
  - [6.3.3 大模型代码示例](#633-大模型代码示例)
  - [8. 相关主题推荐阅读](#8-相关主题推荐阅读)

---

## 6.3.1 Transformer架构Mermaid图

```mermaid
graph TD
    A[输入嵌入] --> B[位置编码]
    B --> C[多头自注意力]
    C --> D[残差连接]
    D --> E[层归一化]
    E --> F[前馈网络]
    F --> G[残差连接]
    G --> H[层归一化]
    H --> I{还有层?}
    I -->|是| C
    I -->|否| J[输出层]

    subgraph "多头注意力"
        K[Query] --> L[注意力计算]
        M[Key] --> L
        N[Value] --> L
        L --> O[多头拼接]
        O --> P[线性变换]
    end
```css
---

## 6.3.2 深度学习LaTeX公式

**注意力机制**:
$$
/text{Attention}(Q, K, V) = /text{softmax}/left(/frac{QK^T}{/sqrt{d_k}}/right)V
$$

**多头注意力**:
$$
/text{MultiHead}(Q, K, V) = /text{Concat}(/text{head}_1, /ldots, /text{head}_h)W^O
$$

其中：
$$
/text{head}_i = /text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

**位置编码**:
$$
PE_{(pos, 2i)} = /sin(pos / 10000^{2i/d_{model}})
$$

$$
PE_{(pos, 2i+1)} = /cos(pos / 10000^{2i/d_{model}})
$$

**Layer Normalization**:
$$
/text{LayerNorm}(x) = /gamma /odot /frac{x - /mu}{/sqrt{/sigma^2 + /epsilon}} + /beta
$$

**损失函数**:
$$
/mathcal{L} = -/sum_{i=1}^{N} y_i /log(/hat{y}_i)
$$

**梯度裁剪**:
$$
g = g /cdot /min(1, /frac{/text{clip/_norm}}{/|g/|_2})
$$

---

## 6.3.3 大模型代码示例

**PyTorch Transformer实现**:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)

        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        output = torch.matmul(attention_weights, V)
        return output, attention_weights

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # 线性变换并重塑
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # 注意力计算
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)

        # 重塑并连接
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )

        # 输出线性变换
        output = self.W_o(attention_output)

        return output, attention_weights

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # 自注意力
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff,
                 max_len=5000, dropout=0.1):
        super(Transformer, self).__init__()

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)
        self.output_layer = nn.Linear(d_model, vocab_size)

    def generate_mask(self, src, tgt):
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)

        seq_len = tgt.size(1)
        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool()
        nopeak_mask = nopeak_mask.to(tgt.device)

        tgt_mask = tgt_mask & nopeak_mask
        return src_mask, tgt_mask

    def forward(self, src, tgt):
        src_mask, tgt_mask = self.generate_mask(src, tgt)

        src_embedded = self.dropout(self.pos_encoding(self.embedding(src)))
        tgt_embedded = self.dropout(self.pos_encoding(self.embedding(tgt)))

        for transformer_block in self.transformer_blocks:
            tgt_embedded = transformer_block(tgt_embedded, tgt_mask)

        output = self.output_layer(tgt_embedded)
        return output

# 使用示例
vocab_size = 30000
d_model = 512
num_heads = 8
num_layers = 6
d_ff = 2048

model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff)
print(f"模型参数数量: {sum(p.numel() for p in model.parameters()):,}")
```text
**LoRA微调实现**:
```python
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import peft
from peft import LoraConfig, get_peft_model

class LoRALinear(nn.Module):
    def __init__(self, linear_layer, rank=16, alpha=32, dropout=0.1):
        super().__init__()
        self.linear = linear_layer
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        # LoRA参数
        self.lora_A = nn.Parameter(torch.randn(rank, linear_layer.in_features) * 0.02)
        self.lora_B = nn.Parameter(torch.zeros(linear_layer.out_features, rank))
        self.lora_dropout = nn.Dropout(dropout)

        # 冻结原始参数
        for param in self.linear.parameters():
            param.requires_grad = False

    def forward(self, x):
        # 原始前向传播
        original_output = self.linear(x)

        # LoRA前向传播
        lora_output = self.lora_dropout(x) @ self.lora_A.T
        lora_output = lora_output @ self.lora_B.T

        return original_output + self.scaling * lora_output

def apply_lora_to_model(model, rank=16, alpha=32, dropout=0.1):
    """将LoRA应用到模型的线性层"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # 跳过某些层（如输出层）
            if 'lm_head' in name or 'output' in name:
                continue

            # 替换为LoRA层
            lora_layer = LoRALinear(module, rank, alpha, dropout)
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]

            if parent_name:
                parent_module = model.get_submodule(parent_name)
                setattr(parent_module, child_name, lora_layer)
            else:
                setattr(model, child_name, lora_layer)

    return model

# 使用PEFT库的LoRA
def setup_lora_with_peft(model_name="microsoft/DialoGPT-medium"):
    # 加载模型和分词器
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # 配置LoRA
    lora_config = LoraConfig(
        r=16,  # LoRA rank
        lora_alpha=32,  # LoRA alpha
        target_modules=["q_proj", "v_proj"],  # 目标模块
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # 应用LoRA
    model = get_peft_model(model, lora_config)

    return model, tokenizer

# 训练函数
def train_lora_model(model, tokenizer, train_data, epochs=3):
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        total_loss = 0
        for batch in train_data:
            # 准备输入
            inputs = tokenizer(batch['text'], return_tensors='pt',
                             truncation=True, padding=True)

            # 前向传播
            outputs = model(**inputs, labels=inputs['input_ids'])
            loss = outputs.loss

            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}, Average Loss: {total_loss/len(train_data):.4f}")

# 使用示例
if __name__ == "__main__":
    model, tokenizer = setup_lora_with_peft()

    # 打印可训练参数
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"可训练参数: {trainable_params:,}")
    print(f"总参数: {total_params:,}")
    print(f"参数减少比例: {(1 - trainable_params/total_params)*100:.2f}%")
```text
**KV Cache优化**:
```python
import torch
import torch.nn as nn
from typing import Optional, Tuple

class KVCache:
    def __init__(self, max_batch_size, max_seq_len, num_heads, head_dim, dtype=torch.float16):
        self.max_batch_size = max_batch_size
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.head_dim = head_dim

        # 预分配缓存
        self.k_cache = torch.zeros(
            max_batch_size, max_seq_len, num_heads, head_dim,
            dtype=dtype, device='cuda'
        )
        self.v_cache = torch.zeros(
            max_batch_size, max_seq_len, num_heads, head_dim,
            dtype=dtype, device='cuda'
        )

        # 当前序列长度
        self.current_lengths = torch.zeros(max_batch_size, dtype=torch.long, device='cuda')

    def update(self, batch_idx: int, seq_idx: int, k: torch.Tensor, v: torch.Tensor):
        """更新KV缓存"""
        self.k_cache[batch_idx, seq_idx] = k
        self.v_cache[batch_idx, seq_idx] = v
        self.current_lengths[batch_idx] = seq_idx + 1

    def get_kv(self, batch_idx: int, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """获取KV缓存"""
        return (
            self.k_cache[batch_idx, :seq_len],
            self.v_cache[batch_idx, :seq_len]
        )

    def clear(self, batch_idx: Optional[int] = None):
        """清除缓存"""
        if batch_idx is None:
            self.current_lengths.zero_()
        else:
            self.current_lengths[batch_idx] = 0

class OptimizedMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None, kv_cache=None,
                batch_idx=None, seq_idx=None):
        batch_size = query.size(0)
        seq_len = query.size(1)

        # 线性变换
        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # 使用KV缓存
        if kv_cache is not None and batch_idx is not None and seq_idx is not None:
            # 更新缓存
            kv_cache.update(batch_idx, seq_idx, K.squeeze(0), V.squeeze(0))

            # 获取完整的KV序列
            cached_k, cached_v = kv_cache.get_kv(batch_idx, seq_idx + 1)
            K = cached_k.unsqueeze(0).transpose(1, 2)
            V = cached_v.unsqueeze(0).transpose(1, 2)

        # 注意力计算
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        output = torch.matmul(attention_weights, V)

        # 重塑并连接
        output = output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )

        # 输出线性变换
        output = self.W_o(output)

        return output, attention_weights

# 使用示例
def generate_with_kv_cache(model, tokenizer, prompt, max_length=100):
    # 初始化KV缓存
    kv_cache = KVCache(
        max_batch_size=1,
        max_seq_len=max_length,
        num_heads=8,
        head_dim=64
    )

    # 编码输入
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    current_length = input_ids.size(1)

    generated_ids = input_ids.clone()

    for i in range(max_length - current_length):
        # 前向传播（使用KV缓存）
        with torch.no_grad():
            outputs = model(
                input_ids=generated_ids,
                kv_cache=kv_cache,
                batch_idx=0,
                seq_idx=i + current_length - 1
            )

        # 获取下一个token
        next_token_logits = outputs.logits[:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1)

        # 添加到生成序列
        generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0)], dim=1)

        # 检查结束条件
        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated_ids[0])
```text
**Flash Attention实现**:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

def flash_attention_forward(q, k, v, mask=None, dropout_p=0.0, softmax_scale=None):
    """
    Flash Attention的前向传播
    参考: https://arxiv.org/abs/2205.14135
    """
    if softmax_scale is None:
        softmax_scale = 1.0 / math.sqrt(q.size(-1))

    batch_size, num_heads, seq_len, head_dim = q.shape

    # 重塑为2D
    q = q.view(batch_size * num_heads, seq_len, head_dim)
    k = k.view(batch_size * num_heads, seq_len, head_dim)
    v = v.view(batch_size * num_heads, seq_len, head_dim)

    if mask is not None:
        mask = mask.view(batch_size * num_heads, seq_len, seq_len)

    # 计算注意力分数
    scores = torch.matmul(q, k.transpose(-2, -1)) * softmax_scale

    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 应用softmax
    attention_weights = F.softmax(scores, dim=-1)

    if dropout_p > 0.0:
        attention_weights = F.dropout(attention_weights, p=dropout_p, training=True)

    # 计算输出
    output = torch.matmul(attention_weights, v)

    # 重塑回原始形状
    output = output.view(batch_size, num_heads, seq_len, head_dim)

    return output, attention_weights

class FlashMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = dropout

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        seq_len = query.size(1)

        # 线性变换
        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # 使用Flash Attention
        attention_output, attention_weights = flash_attention_forward(
            Q, K, V, mask, self.dropout
        )

        # 重塑并连接
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )

        # 输出线性变换
        output = self.W_o(attention_output)

        return output, attention_weights

# 性能比较
def benchmark_attention_methods(batch_size=2, seq_len=512, num_heads=8, head_dim=64):
    """比较不同注意力方法的性能"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 创建输入
    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)
    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)
    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device)

    # 标准注意力
    def standard_attention():
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        return torch.matmul(attention_weights, v)

    # Flash Attention
    def flash_attention():
        return flash_attention_forward(q, k, v)[0]

    # 基准测试
    import time

    # 预热
    for _ in range(10):
        standard_attention()
        flash_attention()

    torch.cuda.synchronize()

    # 标准注意力时间
    start_time = time.time()
    for _ in range(100):
        standard_attention()
    torch.cuda.synchronize()
    standard_time = time.time() - start_time

    # Flash Attention时间
    start_time = time.time()
    for _ in range(100):
        flash_attention()
    torch.cuda.synchronize()
    flash_time = time.time() - start_time

    print(f"标准注意力时间: {standard_time:.4f}s")
    print(f"Flash Attention时间: {flash_time:.4f}s")
    print(f"加速比: {standard_time/flash_time:.2f}x")

# 使用示例
if __name__ == "__main__":
    benchmark_attention_methods()
```

---

## 8. 相关主题推荐阅读

- [2.1 前端主流框架](6.人工智能原理与算法/../2.技术栈与框架/2.1 前端主流框架.md)
- [2.3 Rust前端全栈](6.人工智能原理与算法/../2.技术栈与框架/2.3 Rust前端全栈.md)
- [3.1 Rust](6.人工智能原理与算法/../3.编程语言范式/3.1 Rust.md)
- [6.1 AI基础原理](6.人工智能原理与算法/6.1 AI基础原理.md)
- [6.2 经典AI算法与模型](6.人工智能原理与算法/6.2 经典AI算法与模型.md)
- [6.4 AI工程实践与伦理](6.人工智能原理与算法/6.4 AI工程实践与伦理.md)
- [6.5 AI与哲学](6.人工智能原理与算法/6.5 AI与哲学.md)
- [6.6 AI与认知科学](6.人工智能原理与算法/6.6 AI与认知科学.md)

---

> 本文档持续递归优化，欢迎补充最新技术与学术内容。
